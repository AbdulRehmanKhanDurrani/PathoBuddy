# rag_module.py
import os
import json
import requests
import faiss
import numpy as np
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from time import sleep

# ‚îÄ‚îÄ‚îÄ Configuration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
PDF_FOLDER = "/content/drive/MyDrive/Colab Notebooks/PathoBuddy/data/pdfs"
INDEX_DIR = "/content/drive/MyDrive/Colab Notebooks/PathoBuddy/embeddings/faiss_index"
API_KEY = os.getenv("GOOGLE_API_KEY")

# ‚úÖ CORRECTED Gemini embeddings endpoint
EMBED_URL = f"https://generativelanguage.googleapis.com/v1beta/models/embedding-001:embedContent?key={API_KEY}"

# ‚îÄ‚îÄ‚îÄ Helper: Get Embedding ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def get_embedding(text: str) -> list[float]:
    """Get embedding from Gemini API with correct format"""
    # Truncate to avoid exceeding request size limits
    if len(text) > 2048:  # Gemini embedding limit is ~2048 tokens
        text = text[:2048]
    
    # ‚úÖ CORRECT payload format for Gemini
    payload = {
        "model": "models/embedding-001",
        "content": {
            "parts": [{"text": text}]
        }
    }
    
    headers = {
        "Content-Type": "application/json"
    }
    
    try:
        resp = requests.post(EMBED_URL, json=payload, headers=headers, timeout=30)
        
        if resp.status_code != 200:
            print(f"‚ùå API Response: {resp.status_code} - {resp.text}")
            raise RuntimeError(f"Embedding API Error {resp.status_code}: {resp.text}")
        
        data = resp.json()
        
        # ‚úÖ CORRECT response parsing for Gemini
        if "embedding" in data and "values" in data["embedding"]:
            return data["embedding"]["values"]
        else:
            print(f"‚ùå Unexpected response format: {data}")
            raise RuntimeError(f"Unexpected response format: {data}")
            
    except requests.exceptions.RequestException as e:
        # Simple retry on failure
        print(f"‚ö†Ô∏è Request failed: {e}. Retrying after 5 seconds...")
        sleep(5)
        resp = requests.post(EMBED_URL, json=payload, headers=headers, timeout=30)
        if resp.status_code != 200:
            raise RuntimeError(f"Embedding API Error after retry: {resp.status_code}: {resp.text}")
        data = resp.json()
        if "embedding" in data and "values" in data["embedding"]:
            return data["embedding"]["values"]
        else:
            raise RuntimeError(f"Unexpected response format after retry: {data}")

# ‚îÄ‚îÄ‚îÄ 1) Build the FAISS Index ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def build_index(pdf_folder=PDF_FOLDER, index_dir=INDEX_DIR):
    """Build FAISS index from PDFs"""
    print("üìÇ Loading PDF documents...")
    docs = []
    
    # Check if PDF folder exists
    if not os.path.exists(pdf_folder):
        raise RuntimeError(f"PDF folder not found: {pdf_folder}")
    
    pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith(".pdf")]
    if not pdf_files:
        raise RuntimeError(f"No PDF files found in: {pdf_folder}")
    
    print(f"üìÑ Found {len(pdf_files)} PDF files")
    
    for fn in pdf_files:
        try:
            loader = PyPDFLoader(os.path.join(pdf_folder, fn))
            docs.extend(loader.load())
            print(f"‚úÖ Loaded: {fn}")
        except Exception as e:
            print(f"‚ùå Failed to load {fn}: {e}")

    if not docs:
        raise RuntimeError("No documents loaded successfully")

    print("‚úÇÔ∏è Splitting documents into chunks...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
    chunks = splitter.split_documents(docs)
    
    print(f"üìù Created {len(chunks)} chunks")

    embeddings, texts = [], []
    print(f"üîÑ Processing {len(chunks)} chunks for embeddings...")
    
    for idx, doc in enumerate(chunks, 1):
        txt = doc.page_content.strip()
        # Clean text to remove artifacts
        txt = '\n'.join(line for line in txt.splitlines() if len(line.strip()) > 5)  # Remove short/noisy lines
        txt = txt.replace("~~", "").replace("\f", "").replace("~~~~~", "")  # Remove common PDF artifacts
        if not txt or len(txt) < 10:  # Skip very short chunks
            continue
            
        try:
            print(f"Processing chunk {idx}/{len(chunks)}", end="\r")
            emb = get_embedding(txt)
            embeddings.append(emb)
            texts.append(txt)
            
        except Exception as e:
            print(f"\n‚ùå Embedding failed for chunk {idx}: {e}")
            continue

    if not embeddings:
        raise RuntimeError("‚ùå No embeddings generated; check API key and documents.")

    print(f"\n‚úÖ Generated {len(embeddings)} embeddings")
    
    # Build FAISS index
    print("üèóÔ∏è Building FAISS index...")
    dim = len(embeddings[0])
    index = faiss.IndexFlatL2(dim)
    matrix = np.array(embeddings, dtype="float32")
    index.add(matrix)

    # Save index and texts
    os.makedirs(index_dir, exist_ok=True)
    faiss.write_index(index, os.path.join(index_dir, "index.faiss"))
    
    with open(os.path.join(index_dir, "texts.json"), "w", encoding="utf-8") as f:
        json.dump(texts, f, ensure_ascii=False, indent=2)
    
    print(f"üíæ FAISS index saved at: {index_dir}")
    print(f"üìä Index contains {len(texts)} chunks with {dim}-dimensional embeddings")

# ‚îÄ‚îÄ‚îÄ 2) Load the Index ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def load_index(index_dir=INDEX_DIR):
    """Load existing FAISS index"""
    index_path = os.path.join(index_dir, "index.faiss")
    texts_path = os.path.join(index_dir, "texts.json")
    
    if not os.path.exists(index_path) or not os.path.exists(texts_path):
        raise RuntimeError(f"Index not found at {index_dir}. Run build_index() first.")
    
    idx = faiss.read_index(index_path)
    with open(texts_path, encoding="utf-8") as f:
        texts = json.load(f)
    
    return idx, texts

# ‚îÄ‚îÄ‚îÄ 3) Retrieve Relevant Snippets ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def retrieve_relevant(question: str, k=5, index_dir=INDEX_DIR) -> list[str]:  # Increased k for more context
    """Retrieve top-k relevant text chunks for a question"""
    print(f"üîç Searching for: {question}")
    
    # Get query embedding
    qvec = np.array([get_embedding(question)], dtype="float32")
    
    # Load index and search
    idx, texts = load_index(index_dir)
    dists, ids = idx.search(qvec, k)
    
    results = []
    for i, dist in zip(ids[0], dists[0]):
        if i != -1:  # Valid result
            results.append(texts[i])
    
    print(f"‚úÖ Found {len(results)} relevant chunks")
    return results

# ‚îÄ‚îÄ‚îÄ Test API Connection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def test_api():
    """Test if Gemini API is working"""
    if not API_KEY:
        raise ValueError("‚ùå GOOGLE_API_KEY environment variable not set!")
    
    print("üß™ Testing Gemini API connection...")
    try:
        test_emb = get_embedding("This is a test sentence for pathology.")
        print(f"‚úÖ API working! Embedding dimension: {len(test_emb)}")
        return True
    except Exception as e:
        print(f"‚ùå API test failed: {e}")
        return False

# ‚îÄ‚îÄ‚îÄ Quick Test ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if __name__ == "__main__":
    # Test API first
    if not test_api():
        print("‚ùå Fix API issues before building index")
        exit(1)
    
    print("\nüèóÔ∏è Building FAISS index...")
    try:
        build_index()
        print("\nüîç Testing retrieval...")
        results = retrieve_relevant("Which stain is used for tuberculosis diagnosis?", k=5)
        
        print("\nüìã Retrieved snippets:")
        for i, snippet in enumerate(results, 1):
            print(f"\n--- Snippet {i} ---")
            print(snippet[:200] + "..." if len(snippet) > 200 else snippet)
            
    except Exception as e:
        print(f"‚ùå Error: {e}")